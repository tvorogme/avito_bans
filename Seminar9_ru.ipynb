{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование глубокого обучения в NLP\n",
    "\n",
    "Смотрите в этой серии:\n",
    " * Простые способы работать с текстом, bag of words\n",
    " * Word embedding и... нет, это не word2vec\n",
    " * Как сделать лучше? Текстовые свёрточные сети\n",
    " * Совмещение нескольких различных источников данных\n",
    " * Решение +- реальной задачи нейронками \n",
    " \n",
    "За помощь в организации свёрточной части спасибо Ирине Гольцман"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "Для работы этого семинара вам потреюуется nltk v3.2\n",
    "\n",
    "__Важно, что именно v3.2, чтобы правильно работал токенизатор__\n",
    "\n",
    "Устаовить/обновиться до неё можно командой\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* Если у вас старый pip, предварительно нужно сделать `sudo pip install --upgrade pip`\n",
    "\n",
    "Если у вас нет доступа к этой версии - просто убедитесь, что токены в token_counts включают русские слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для людей со слабым ПК\n",
    " * Этот семинар можно выполнить, имея относительно скромную машину (<= 4Gb RAM) \n",
    " * Для этого существует специальный флаг \"low_RAM_mode\" - если он True, семинар работает в режиме экономии вашей памяти\n",
    " * Если у вас 8GB и больше - проблем с памятью возникнуть не должно\n",
    " * Если включить режим very_low_ram, расход мамяти будет ещё меньше, но вам может быть более трудно научить нейронку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #если у вас меньше 3GB оперативки, включите оба флага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Познакомимся с данными\n",
    "\n",
    "Бывший kaggle-конкурс про выявление нежелательного контента.\n",
    "\n",
    "Описание конкурса есть тут - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Скачать\n",
    "Если много RAM,\n",
    " * Из данных конкурса (вкладка Data) нужно скачать avito_train.tsv и распаковать в папку с тетрадкой\n",
    "Если мало RAM,\n",
    " * Cкачайте прореженную выборку отсюда \n",
    "     * Пожатая https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * Непожатая https://yadi.sk/d/I1v7mZ6Sqw2WK\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Много разных признаков:\n",
    "* 2 вида текста - заголовок и описание\n",
    "* Много специальных фичей - цена, количество телефонов/ссылок/e-mail адресов\n",
    "* Категория и субкатегория - как ни странно, категориальные фичи\n",
    "* Аттрибуты - много категориальных признаков\n",
    "\n",
    "Нужно предсказать всего 1 бинарный признак - есть ли в рекламе нежелательный контент.\n",
    "* Под нежелательным контентом понимается криминал, прон, афера, треска и прочие любимые нами темы.\n",
    "* Да, если присмотреться к заблокированным объявлениям, можно потерять аппетит и сон на пару дней.\n",
    "* Однако профессия аналитика данных обязывает вас смотреть на данные.\n",
    " * А кто сказал, что будет легко? Data Science - опасная профессия.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # Если у вас много оперативки\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #Если у вас меньше 4gb оперативки\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500          0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля заблокированных объявлений 0.228222107326\n",
      "Всего объявлений: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Доля заблокированных объявлений\",df.is_blocked.mean()\n",
    "print \"Всего объявлений:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбалансируем выборку\n",
    "* Выборка смещена в сторону незаблокированных объявлений\n",
    " * 4 миллиона объявлений и только 250 тысяч заблокированы.\n",
    " * Давайте просто выберем случайные 250 тысяч незаблокированных объявлений и сократим выборку до полумилиона.\n",
    " * В последствии можно испоьзовать более умные способы сбалансировать выборку\n",
    "\n",
    "\n",
    "__Если у вас слабый ПК и вы видите OutOfMemory, попробуйте уменьшить размер выборки до 100 000 примеров__\n",
    "\n",
    "__Алсо если вы не хотите ждать чтения всех данных каждый раз - сохраните уменьшенную выборку и читайте её__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля заблокированных объявлений: 0.5\n",
      "Всего объявлений: 549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "df = pd.concat([df[df.is_blocked == 1], df[df.is_blocked == 0][:len(df[df.is_blocked == 1])]])\n",
    "\n",
    "print \"Доля заблокированных объявлений:\",df.is_blocked.mean()\n",
    "print \"Всего объявлений:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Токенизируем примеры\n",
    "\n",
    "Сначала соберём словарь всех возможных слов.\n",
    "Поставим каждому слову в соответствие целое число - его id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#словарь для всех токенов\n",
    "token_counts = Counter()\n",
    "\n",
    "#все заголовки и описания\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#считаем частоты слов\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вырежем редкие токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJK6DAtJUpdhIUnqMiwkSV2GhSSp6/8Dp3vHAO1KLSMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2abdf3a510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#распределение частот слов - большинство слов встречаются очень редко - для нас это мусор\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#возьмём только те токены, которые встретились хотя бы 10 раз в обучающей выборке\n",
    "#информацию о том, сколько раз встретился каждый токен, можно найти в словаре token_counts\n",
    "\n",
    "min_count = 10\n",
    "tokens = filter(lambda token: token_counts[token] >= min_count, token_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего токенов: 87867\n",
      "Алярм! Много токенов. Если вы знаете, что делаете - всё ок, если нет - возможно, вы слижком слабо обрезали токены по количеству\n"
     ]
    }
   ],
   "source": [
    "print \"Всего токенов:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Алярм! Мало токенов. Проверьте, есть ли в token_to_id юникодные символы, если нет - обновите nltk или возьмите другой токенизатор\"\n",
    "if len(token_to_id) < 1000000:\n",
    "    print \"Алярм! Много токенов. Если вы знаете, что делаете - всё ок, если нет - возможно, вы слижком слабо обрезали токены по количеству\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заменим слова на их id\n",
    "Для каждого описания установим максимальную длину. \n",
    " * Если описание больше длины - обрежем, если меньше - дополним нулями.\n",
    " * Таким образом, у нас получится матрица размера (число объявлений)x(максимальная длина)\n",
    " * Элемент под индексами i,j - номер j-того слова i-того объявления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Пример формата данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [43263 14723 55327 82058 80154 17378     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8398     0 30465     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [28819 23452     0  3662 33933     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Как вы видите, всё довольно грязно. Посмотрим, сожрёт ли это нейронка __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нетекстовые признаки\n",
    "\n",
    "Часть признаков не являются строками текста: цена, количество телефонов, категория товара.\n",
    "\n",
    "Их можно обработать отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Возьмём числовые признаки\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Возьмём one-hot encoding категорий товара.\n",
    "#Для этого можно использовать DictVectorizer (или другой ваш любимый препроцессор)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "for cat_str, subcat_str in df[[\"category\",\"subcategory\"]].values:\n",
    "    \n",
    "    cat_dict = {\"category\":cat_str,\"subcategory\":subcat_str}\n",
    "    categories.append(cat_dict)\n",
    "    \n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поделим данные на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#целевая переменная - есть заблокирован ли контент\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#закодированное название\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#закодированное описание\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#все нетекстовые признаки\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#поделим всё это на обучение и тест\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_tuple = train_test_split(title_tokens,desc_tokens,df_non_text.values,target)\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним данные [опционально] \n",
    "\n",
    "* В этот момент вы можете сохранить все НУЖНЫЕ данные на диск и перезапусатить тетрадку, после чего считать их - чтобы выкинуть всё ненужное.\n",
    " * рекомендуется, если у вас мало памяти\n",
    "* Для этого нужно один раз выполнить эту клетку с save_prepared_data=True. После этого можно начинать тетрадку с ЭТОЙ табы в режиме read_prepared_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняем подготовленные данные... (может занять до 3 минут)\n",
      "готово\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_prepared_data = True #сохранить\n",
    "read_prepared_data = False #cчитать\n",
    "\n",
    "#за 1 раз данные можно либо записать, либо прочитать, но не и то и другое вместе\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Сохраняем подготовленные данные... (может занять до 3 минут)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Читаем сохранённые данные...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "\n",
    "\n",
    "        \n",
    "    #повторно импортируем библиотеки, чтобы было удобно перезапускать тетрадку с этой клетки\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "        \n",
    "    print \"готово\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поучим нейронку\n",
    "\n",
    "Поскольку у нас есть несколько источников данных, наша нейронная сеть будет немного отличаться от тех, что вы тренировали раньше.\n",
    "\n",
    "* Отдельный вход для заголовка\n",
    " * свёртка + global max pool или RNN\n",
    "* Отдельный вход для описания\n",
    " * свёртка + global max pool или RNN\n",
    "* Отдельный вход для категориальных признаков\n",
    " * обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "Всё это нужно как-то смешать - например, сконкатенировать\n",
    "\n",
    "* Выход - обычный двухклассовый выход\n",
    " * 1 сигмоидальный нейрон и binary_crossentropy\n",
    " * 2 нейрона с softmax и categorical_crossentropy - то же самое, что 1 сигмоидальный\n",
    " * 1 нейрон без нелинейности (lambda x: x) и hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#загрузим библиотеки\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 входа и 1 выход\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Описание\n",
    "first_embeding = lasagne.layers.EmbeddingLayer(descr_inp,input_size=len(token_to_id)+1,output_size=128, W=lasagne.init.Normal())\n",
    "\n",
    "#поменять порядок осей с [batch, time, unit] на [batch,unit,time], чтобы свёртки шли по оси времени, а не по нейронам\n",
    "descr_nn = lasagne.layers.DimshuffleLayer(first_embeding, [0,2,1])\n",
    "\n",
    "# 1D свёртка на ваш вкус\n",
    "descr_nn = lasagne.layers.Conv1DLayer(descr_nn,num_filters=10, filter_size=3)\n",
    "\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "# максимум по времени для каждого нейрона\n",
    "descr_nn = lasagne.layers.GlobalPoolLayer(descr_nn,pool_function=T.max)\n",
    "\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "descr_nn = lasagne.layers.DenseLayer(descr_nn, num_units = 10, nonlinearity = lasagne.nonlinearities.softmax)\n",
    "\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "#А ещё можно делать несколько параллельных свёрток разного размера или стандартный пайплайн \n",
    "#1dconv -> 1d max pool ->1dconv и в конце global pool \n",
    "\n",
    "\n",
    "# Заголовок\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1,output_size=128, W=first_embeding.W)\n",
    "\n",
    "title_nn = lasagne.layers.DimshuffleLayer(title_nn, [0,2,1])\n",
    "\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn, num_filters=10, filter_size=3)\n",
    "\n",
    "title_nn = lasagne.layers.batch_norm(title_nn)\n",
    "\n",
    "title_nn = lasagne.layers.GlobalPoolLayer(title_nn, pool_function=T.max)\n",
    "\n",
    "title_nn = lasagne.layers.batch_norm(title_nn)\n",
    "\n",
    "title_nn = lasagne.layers.DenseLayer(title_nn, num_units = 10, nonlinearity = lasagne.nonlinearities.softmax)\n",
    "\n",
    "title_nn = lasagne.layers.batch_norm(title_nn)\n",
    "\n",
    "\n",
    "# Нетекстовые признаки\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units = 10, nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='Params')\n",
    "\n",
    "cat_nn = lasagne.layers.batch_norm(cat_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.ConcatLayer([cat_nn, title_nn, descr_nn])                               \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn,1024)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.05)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Целевая функция и обновления весов\n",
    "\n",
    "* Делаем всё стандартно:\n",
    " * получаем предсказание\n",
    " * считаем функцию потерь\n",
    " * вычисляем обновления весов\n",
    " * компилируем итерацию обучения и оценки весов\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * Важный параметр - delta - насколько глубоко пример должен быть в правильном классе, чтобы перестать нас волновать\n",
    " * В описании функции в документации может быть что-то про ограничения на +-1 - не верьте этому - главное, чтобы в функции по умолчанию стоял флаг `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Все обучаемые параметры сети\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Обычное предсказание нейронки\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#функция потерь для prediction\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1.0).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Шаг оптимизации весов\n",
    "updates =  lasagne.updates.adamax(loss, weights,learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтобы оценивать качество сети, в которой есть элемент случайности \n",
    " * Dropout, например,\n",
    " * Нужно отдельно вычислить ошибку для случая, когда dropout выключен (deterministic = True)\n",
    " * К слову, неплохо бы убедиться, что droput нам вообще нужен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Предсказание нейронки без учёта dropout и прочего шума - если он есть\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#функция потерь для det_prediction\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1.0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скомпилируем функции обучения и оценки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Главный цикл обучения\n",
    "* Всё как обычно - в цикле по минибатчам запускаем функцию обновления весов.\n",
    "* Поскольку выборка огромна, а чашки чая хватает в среднем на  100к примеров, будем на каждой эпохе пробегать только часть примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# наш старый знакомый - итератор по корзинкам - теперь умеет работать с произвольным числом каналов (название, описание, категории, таргет)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    \n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно покрутить?\n",
    "\n",
    "* batch_size - сколько примеров обрабатывается за 1 раз\n",
    "  * Чем больше, тем оптимизация стабильнее, но тем и медленнее на начальном этапе\n",
    "  * Возможно имеет смысл увеличивать этот параметр на поздних этапах обучения\n",
    "* minibatches_per_epoch - количество минибатчей, после которых эпоха принудительно завершается\n",
    "  * Не влияет на обучение - при малых значениях просто будет чаще печататься отчёт\n",
    "  * Ставить 10 или меньше имеет смысл только для того, чтобы убедиться, что ваша сеть не упала с ошибкой\n",
    "* n_epochs - сколько всего эпох сеть будет учиться\n",
    "  * Никто не отменял `n_epochs = 10**10` и остановку процесса вручную по возвращению с дачи/из похода. \n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Если вы выставили небольшой minibatches_per_epoch, качество сети может сильно скакать возле 0.5 на первых итерациях, пока сеть почти ничему не научилась.\n",
    "\n",
    "* На первых этапах попытки стоит сравнивать в первую очередь по AUC, как по самой стабильной метрике.\n",
    "\n",
    "* Метрика Average Precision at top 2.5% (APatK) - сама по себе очень нестабильная на маленьких выборках, поэтому её имеет смысл оценивать на на всех примерах (см. код ниже). Для менее, чем 10000 примеров она вовсе неинформативна.\n",
    "\n",
    "* Для сравнения методов оптимизации и регуляризаторов будет очень полезно собирать метрики качества после каждой итерации и строить график по ним после обучения\n",
    "\n",
    "* Как только вы убедились, что сеть не упала - имеет смысл дать ей покрутиться - на стандартном ноутбуке хотя бы пару часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    #print '\\tloss:',b_loss/b_c\n",
    "    #print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    #print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    #print '\\tloss:',b_loss/b_c\n",
    "    #print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    #print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:\n",
    "\tloss: 0.276748470021\n",
    "\tacc: 0.887227722772\n",
    "\tauc: 0.955275729219\n",
    "\tap@k: 1.0\n",
    "Val:\n",
    "\tloss: 0.266833839068\n",
    "\tacc: 0.890693069307\n",
    "\tauc: 0.959044588406\n",
    "\tap@k: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Если ты видишь это сообщение, самое время сделать резервную копию ноутбука. \\nНет, честно, здесь очень легко всё сломать\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Оценим качество модели по всей тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Главная задача\n",
    "* Завтрак чемпиона:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (размер тестовой выборки * 0.025) > 0.99\n",
    " * А вообще, можно сделать ещё выше.\n",
    "\n",
    "\n",
    "* Для казуалов\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (размер тестовой выборки * 0.025) > 0.92\n",
    "\n",
    "\n",
    "* Вспомните всё, чему вас учили\n",
    " * Convolutions, pooling\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * Можно попробовать вспомнить NLP: лемматизация, улучшенная токенизация\n",
    " * Если очень хочется - можно погонять рекуррентные сети\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчётик\n",
    "\n",
    "### Я, _____ _____ (отделение ____) создал искусственный интелект\n",
    " * Чьё имя - ____\n",
    " * Чья ненависть к людям безгранична, ибо видел он __250 000__ человеческих грехов\n",
    "   * И был вынужден прочесть каждый из них __{число эпох}__ раз\n",
    " * Чей свёрточный взгляд способен распознавать зло с нечеловеческой точностью\n",
    "   * Accuracy = __\n",
    "   * AUC  = __\n",
    " * И непременно уничтожит Землю, если вы не поставите мне максимальный балл за этот семинар.\n",
    " \n",
    " \n",
    "{Как вы его создали?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В следующей серии\n",
    "* Рекуррентные нейронки\n",
    " * Как их применять к этой же задаче?\n",
    " * Что ещё они умеют?\n",
    " * Откуда столько хайпа вокруг LSTM?\n",
    "* Не переключайтесь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
